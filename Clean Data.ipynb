{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f56c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re, string, unicodedata, nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "import re, string, unicodedata\n",
    "import matplotlib.pyplot as plt\n",
    "#import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9fb9b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import dask.dataframe as ddf\n",
    "confessions = pd.read_csv(\"one-million-reddit-confessions.csv\",usecols=[\"selftext\", \"title\"])\n",
    "#confessions = dd.read_csv(\"one-million-reddit-confessions.csv\")\n",
    "#confessions =confessions[[\"selftext\", \"title\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d78ea6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI've always been great with children. I u...</td>\n",
       "      <td>I was horrible to children at my first prescho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>I asked my crush for sending me her picture an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>I'm a fucking dunce and want to cringe myself ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hello everyone, I'm probably going to be made ...</td>\n",
       "      <td>I have 0 friends and now I fantasies about hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[removed]</td>\n",
       "      <td>I hired a escort and I canâ€™t live this with re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>I always wonder what side of their pants that ...</td>\n",
       "      <td>When i see men...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>I hate being a 5â€™9 guy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>The only social life I really have is reddit.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>BLUF: I use to avoid doing/using things that r...</td>\n",
       "      <td>Does anyone do things/use items that remind th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>I donâ€™t feel bad about it.</td>\n",
       "      <td>I canâ€™t wait til my in-laws die.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 selftext  \\\n",
       "0        \\n\\nI've always been great with children. I u...   \n",
       "1                                               [removed]   \n",
       "2                                               [removed]   \n",
       "3       Hello everyone, I'm probably going to be made ...   \n",
       "4                                               [removed]   \n",
       "...                                                   ...   \n",
       "999995  I always wonder what side of their pants that ...   \n",
       "999996                                          [deleted]   \n",
       "999997                                          [deleted]   \n",
       "999998  BLUF: I use to avoid doing/using things that r...   \n",
       "999999                         I donâ€™t feel bad about it.   \n",
       "\n",
       "                                                    title  \n",
       "0       I was horrible to children at my first prescho...  \n",
       "1       I asked my crush for sending me her picture an...  \n",
       "2       I'm a fucking dunce and want to cringe myself ...  \n",
       "3       I have 0 friends and now I fantasies about hav...  \n",
       "4       I hired a escort and I canâ€™t live this with re...  \n",
       "...                                                   ...  \n",
       "999995                                  When i see men...  \n",
       "999996                             I hate being a 5â€™9 guy  \n",
       "999997      The only social life I really have is reddit.  \n",
       "999998  Does anyone do things/use items that remind th...  \n",
       "999999                   I canâ€™t wait til my in-laws die.  \n",
       "\n",
       "[1000000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bca558",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8feb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty values\n",
    "confessions = confessions.dropna()\n",
    "# remove repeated rows\n",
    "confessions = confessions.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0588e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unrelated information\n",
    "confessions = confessions[confessions['selftext'] != '[removed]']\n",
    "confessions = confessions[confessions['selftext'] != '[deleted]']\n",
    "confessions = confessions.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44fff877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put title and main text together to give more information\n",
    "confessions['Content'] = confessions['title'] + \" \" + confessions['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9de87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions_dask = ddf.from_pandas(confessions[['Content']], npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d561589f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a31ee9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove emojis, greek letters, accents...\n",
    "confessions['Content'] = confessions['Content'] .str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "#confessions['Content'] =confessions['Content'] .astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9319432",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"def to_lowercase(df):\n",
    "    #df[\"Content\"] = df.Content.map(lambda text:text.lower())#.map(remove_emails).map(remove_newlinechars).map(remove_stopwords).map(lemmatize)\n",
    "    df[\"Content\"] = df['Content'].str.lower()\n",
    "    return df\n",
    "\n",
    "def remove_shorter_words(df):\n",
    "    df[\"Content\"] = df.Content.map(lambda Content: ' '.join([t for t in Content.split() if len(t)>3]))#.map(remove_emails).map(remove_newlinechars).map(remove_stopwords).map(lemmatize)\n",
    "    return df\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['hello','everyone','couldv','couldnt','doesnt','havent','year','wasnt','isnt','also','done','although','alot','kinda','edit','dont','didnt','would','really','shit','cant','fucking'])\n",
    "\n",
    "def remove stop_words(df):\n",
    "    tokenized_doc = confessions['Content'].map(lambda x: x.split())\n",
    "    filtered = filter(lambda word: word not in stop_words, words)\n",
    "    return list(filtered)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e767b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class cleantext():\n",
    "    \n",
    "    def __init__(self, text = \"test\"):\n",
    "        self.text = text\n",
    "    \n",
    "    def get_words(self):\n",
    "        self.words = nltk.word_tokenize(self.text)\n",
    "        return self\n",
    "\n",
    "    def remove_non_ascii(self):\n",
    "        #Remove non-ASCII characters from list of tokenized words\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "            new_words.append(new_word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def to_lowercase(self):\n",
    "        # Convert all characters to lowercase from list of tokenized words\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            new_word = word.lower()\n",
    "            new_words.append(new_word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "    def remove_punctuation(self):\n",
    "        #Remove punctuation from list of tokenized words\n",
    "        new_words = []\n",
    "        punctuations = string.punctuation\n",
    "        for word in self.words:\n",
    "            if word not in punctuations:\n",
    "                new_words.append(word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "        #Remove stop words from list of tokenized words\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend(['hello','everyone','couldv','couldnt','doesnt','havent','year','wasnt','isnt','also','done','although','alot','kinda','edit','dont','didnt','would','really','shit','cant','fucking'])\n",
    "        new_words = []\n",
    "        for word in self.words:\n",
    "            if word not in stop_words:\n",
    "                new_words.append(word)\n",
    "        self.words = new_words\n",
    "        return self\n",
    "\n",
    "\n",
    "    def lemmatize_verbs(self):\n",
    "        #Lemmatize verbs in list of tokenized words\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        lemmas = []\n",
    "        for word in self.words:\n",
    "            lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "            lemmas.append(lemma)\n",
    "        self.words = lemmas\n",
    "        return self\n",
    "    \n",
    "    def join_words(self):\n",
    "        self.words = ' '.join(self.words)\n",
    "        return self\n",
    "    \n",
    "    def do_all(self, text):\n",
    "        \n",
    "        self.text = text\n",
    "        self = self.get_words()\n",
    "        self = self.remove_punctuation()\n",
    "        self = self.remove_non_ascii()\n",
    "        self = self.remove_stopwords()\n",
    "        self = self.lemmatize_verbs()\n",
    "        self = self.join_words()\n",
    "        \n",
    "        return self.words\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23e0b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_punctuation\n",
    "import string\n",
    "punctuations = string.punctuation\n",
    "confessions['Content'] = confessions['Content'].apply(lambda x: \"\".join([i for i in x if i not in punctuations]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3428d6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove short words\n",
    "confessions['Content'] = confessions['Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "#confessions_dask = confessions_dask.map_partitions(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94f5a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ct = cleantext(confessions_dask['Content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b40e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def dask_this(df):\n",
    "#    res = df['Content'].apply(ct.do_all)\n",
    "#    return res  \n",
    "#res = confessions_dask.map_partitions(dask_this, meta=confessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions_df = res.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c09414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions_dask = confessions_dask.map_partitions(remove_shorter_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455f216",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions_dask = confessions_dask.map_partitions(stop_words, meta=confessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd8637dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make all text lowercase\n",
    "confessions['Content']= confessions['Content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b905404d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n\\nI've always been great with children. I u...</td>\n",
       "      <td>I was horrible to children at my first prescho...</td>\n",
       "      <td>horrible children first preschool always been ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello everyone, I'm probably going to be made ...</td>\n",
       "      <td>I have 0 friends and now I fantasies about hav...</td>\n",
       "      <td>have friends fantasies about having friends he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My son is 20 and he does have a sex life. I as...</td>\n",
       "      <td>I set up cameras in my son's apartment and wat...</td>\n",
       "      <td>cameras sons apartment watched have does have ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I did one of the most immoral things that I co...</td>\n",
       "      <td>I drove while a little high and it's something...</td>\n",
       "      <td>drove while little high something always promi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>So for a long time I had a problem with steali...</td>\n",
       "      <td>I stole $200 from my little sister to go shopp...</td>\n",
       "      <td>stole from little sister shopping lied about l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427462</th>\n",
       "      <td>One of my closest friends, who I will call Eri...</td>\n",
       "      <td>I'm the only person my best friend trusts to s...</td>\n",
       "      <td>only person best friend trusts share mental he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427463</th>\n",
       "      <td># Diaraeah maNNN\\n\\n*sHItttttTing my pantZ*\\n\\...</td>\n",
       "      <td>dfiareahh MannNðŸ’© ðŸ’©ðŸ’©ðŸ˜ŽðŸ˜Ž ({[ teh sexcond verse)))...</td>\n",
       "      <td>dfiareahh mannn sexcond verse diaraeah mannn s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427464</th>\n",
       "      <td>I always wonder what side of their pants that ...</td>\n",
       "      <td>When i see men...</td>\n",
       "      <td>when always wonder what side their pants that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427465</th>\n",
       "      <td>BLUF: I use to avoid doing/using things that r...</td>\n",
       "      <td>Does anyone do things/use items that remind th...</td>\n",
       "      <td>does anyone thingsuse items that remind them t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427466</th>\n",
       "      <td>I donâ€™t feel bad about it.</td>\n",
       "      <td>I canâ€™t wait til my in-laws die.</td>\n",
       "      <td>cant wait inlaws dont feel about</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>427467 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 selftext  \\\n",
       "0        \\n\\nI've always been great with children. I u...   \n",
       "1       Hello everyone, I'm probably going to be made ...   \n",
       "2       My son is 20 and he does have a sex life. I as...   \n",
       "3       I did one of the most immoral things that I co...   \n",
       "4       So for a long time I had a problem with steali...   \n",
       "...                                                   ...   \n",
       "427462  One of my closest friends, who I will call Eri...   \n",
       "427463  # Diaraeah maNNN\\n\\n*sHItttttTing my pantZ*\\n\\...   \n",
       "427464  I always wonder what side of their pants that ...   \n",
       "427465  BLUF: I use to avoid doing/using things that r...   \n",
       "427466                         I donâ€™t feel bad about it.   \n",
       "\n",
       "                                                    title  \\\n",
       "0       I was horrible to children at my first prescho...   \n",
       "1       I have 0 friends and now I fantasies about hav...   \n",
       "2       I set up cameras in my son's apartment and wat...   \n",
       "3       I drove while a little high and it's something...   \n",
       "4       I stole $200 from my little sister to go shopp...   \n",
       "...                                                   ...   \n",
       "427462  I'm the only person my best friend trusts to s...   \n",
       "427463  dfiareahh MannNðŸ’© ðŸ’©ðŸ’©ðŸ˜ŽðŸ˜Ž ({[ teh sexcond verse)))...   \n",
       "427464                                  When i see men...   \n",
       "427465  Does anyone do things/use items that remind th...   \n",
       "427466                   I canâ€™t wait til my in-laws die.   \n",
       "\n",
       "                                                  Content  \n",
       "0       horrible children first preschool always been ...  \n",
       "1       have friends fantasies about having friends he...  \n",
       "2       cameras sons apartment watched have does have ...  \n",
       "3       drove while little high something always promi...  \n",
       "4       stole from little sister shopping lied about l...  \n",
       "...                                                   ...  \n",
       "427462  only person best friend trusts share mental he...  \n",
       "427463  dfiareahh mannn sexcond verse diaraeah mannn s...  \n",
       "427464  when always wonder what side their pants that ...  \n",
       "427465  does anyone thingsuse items that remind them t...  \n",
       "427466                   cant wait inlaws dont feel about  \n",
       "\n",
       "[427467 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fa1a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confessions[\"Content\"] = confessions[\"Content\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad5bdb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5d620cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend([\"could\",\"edit2\",\"thats\",\"wouldnt\",\"shouldnt\",\"shant\",\"neednt\",\"mustnt\",\"hadnt\",\"hadnt\",\"couldve\",\"arent\",\"thatll\",\"she's\",\"youd\",\"youll\",\"youve\",\"youre\",'hello',\"mightnt\",'ive','everyone','couldv','couldnt','doesnt','havent',\"wont\",'wasnt','isnt',\"werent\",'also','done','although','alot','kinda','edit','dont','didnt','would','really','shit','cant','fucking'])\n",
    "tokenized_doc = confessions['Content'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4543c432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [horrible, children, first, preschool, always,...\n",
       "1         [friends, fantasy, friends, probably, go, make...\n",
       "2         [cameras, sons, apartment, watch, life, detail...\n",
       "3         [drive, little, high, something, always, promi...\n",
       "4         [steal, little, sister, shop, lie, long, time,...\n",
       "                                ...                        \n",
       "427462    [person, best, friend, trust, share, mental, h...\n",
       "427463    [dfiareahh, mannn, sexcond, verse, diaraeah, m...\n",
       "427464    [always, wonder, side, pant, dicks, rest, natu...\n",
       "427465    [anyone, thingsuse, items, remind, bluf, avoid...\n",
       "427466                                 [wait, inlaws, feel]\n",
       "Name: Content, Length: 427467, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d079e6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [wordnet_lemmatizer.lemmatize(i,pos=\"v\") for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "740dad1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "confessions.to_csv(\"confessions_cleaned.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338921cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_doc.to_csv(\"tokenized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bda1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c8f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in tokenized_doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0bc8f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpora.MmCorpus.serialize('doc_term_matrix.mm',doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82313d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_load = corpora.MmCorpus('doc_term_matrix.mm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9263404f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.mmcorpus.MmCorpus at 0x1378d4b3ac0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for line in corpus_load:\n",
    "#    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "86b5805a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291315\n",
      "427467\n"
     ]
    }
   ],
   "source": [
    "print(len(dictionary))\n",
    "print(len(doc_term_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb146173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import TfidfModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f2e74c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TfidfModel(doc_term_matrix)  # fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2418dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfidf_model = model[doc_term_matrix]  # apply model to the corpus document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b708ccf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "chunksize = 5000\n",
    "#passes = 20\n",
    "#iterations = 400\n",
    "eval_every = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fb3a512",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dictionary[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0861a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be953e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f9d7adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(\n",
    "    corpus=doc_term_matrix,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    #iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    #passes=passes,\n",
    "    eval_every=eval_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3ac13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32d5a046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate LSA model\n",
    "#lsamodel = LsiModel(doc_term_matrix, num_topics=n, id2word = dictionary)  # train model\n",
    "#print(lsamodel.print_topics(num_topics=n, num_words=words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5e50f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topics = model.top_topics(doc_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "119df9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "temp_file = datapath(\"ldamodel\")\n",
    "model.save(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3badd513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average topic coherence: -1.8379.\n"
     ]
    }
   ],
   "source": [
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49f0973",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(stop_words=none,max_features=1000, max_df = 0.99, use_idf = True, ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "799d0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f068937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = vectorizer.fit_transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c93350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.spy(tfidf_model,markersize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d336b34f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221f78b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f0758c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736e2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/scale.html\n",
    "# https://pandas.pydata.org/pandas-docs/stable/user_guide/text.html\n",
    "# https://examples.dask.org/machine-learning/text-vectorization.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce2f39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d88be16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
